FROM python:3.12-slim-bookworm

# variáveis para definir a verso do SDK para conectar no minio, hadoop e spark
ARG AWS_SDK_VERSION=1.12.788
ARG HADOOP_VERSION=3.3.4
ARG SPARK_VERSION=3.5.6

# instalação das dependências base
RUN apt update && \
    DEBIAN_FRONTEND=noninteractive apt install -y --no-install-recommends \
    curl \
    procps \
    openjdk-17-jre-headless \
    && rm -rf /var/lib/apt/lists/*

# variáveis de ambiente para definir a versão do spark e url para baixar o spark-hadoop
ENV SPARK_HOME="/opt/spark"
ENV SPARK_TGZ_URL="https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz"

# criação do diretório do spark e extração dos arquivos
RUN mkdir -p /opt/spark && \
    curl -sSL ${SPARK_TGZ_URL} -o /tmp/spark.tgz && \
    tar -xzf /tmp/spark.tgz -C ${SPARK_HOME} --strip-components=1 && \
    rm -rf /tmp/spark.tgz

# definição das variáveis de ambiente dos jars a serem utilizados
ENV AWS_JAVA_SDK_BUNDLE_JAR_URL="https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_SDK_VERSION}/aws-java-sdk-bundle-${AWS_SDK_VERSION}.jar"
ENV HADOOP_AWS_JAR_URL="https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar"

# criação do usuario e do grupo para acesso ao spark
RUN groupadd --system --gid 1000 spark && \
    useradd --system --gid spark --uid 1000 sparkuser && \
    chown -R sparkuser:spark "${SPARK_HOME}"

WORKDIR "${SPARK_HOME}"